{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g2RQRSltgja"
      },
      "source": [
        "# Full DL Solution\n",
        "---\n",
        "### **Case Study:** Stroke Prediction\n",
        "\n",
        "**Objective:** The goal of this project is to walk you through a case study where you can apply the deep learning concepts that you learned about during the week. By the end of this project, you would have developed a solution that predicts if a person will have a stroke or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSCD88NxJjFo"
      },
      "source": [
        "**Dataset Explanation:** We will be using the stroke dataset. Its features are:\n",
        "\n",
        "\n",
        "* **id:** unique identifier\n",
        "* **gender:** \"Male\", \"Female\" or \"Other\"\n",
        "* **age:** age of the patient\n",
        "* **hypertension:** 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
        "* **heart_disease:** 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
        "* **ever_married:** \"No\" or \"Yes\"\n",
        "* **work_type:** \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
        "* **Residence_type:** \"Rural\" or \"Urban\"\n",
        "* **avg_glucose_level:** average glucose level in blood\n",
        "* **bmi:** body mass index\n",
        "* **smoking_status:** \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
        "* **stroke:** 1 if the patient had a stroke or 0 if not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBvX5nCYt8cT"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaPfsfvXK2_0"
      },
      "source": [
        "We start by importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oK92M0m-ezbE"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMQuUG7OtfrG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-RxAH5auFFy"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_jj2t6zK6zy"
      },
      "source": [
        "We load the dataset from a csv file, and see its first rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQVo1CAJt7s8"
      },
      "outputs": [],
      "source": [
        "path = 'healthcare-dataset-stroke-data.csv'\n",
        "data = pd.read_csv(path)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6gAyBGtubI7"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ_93CvuLF3j"
      },
      "source": [
        "Now we start the exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2925yVCdud0a"
      },
      "source": [
        "### Shape of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ0hWvVALJy4"
      },
      "source": [
        "First thing we need to know the shape of our data\n",
        "\n",
        "**Question 1:** How many examples and features do we have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pvWR3PKuQEy"
      },
      "outputs": [],
      "source": [
        "nrow, ncol = data.shape\n",
        "print(nrow, ncol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUy4oI5xukRr"
      },
      "source": [
        "### Types of different Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1q10ievLTTs"
      },
      "source": [
        "**Question 2:** Check the type of each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8snoohouhUP"
      },
      "outputs": [],
      "source": [
        "column_types = data.dtypes\n",
        "print(\"Data types of each feature is :\")\n",
        "print(column_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkrEh6RYygms"
      },
      "source": [
        "### Dealing with categorical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** Use the .value_counts() functions to walk through the categorical variables that we have to see the categories and the counts of each of them."
      ],
      "metadata": {
        "id": "v_kBYl7ItLGt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DlSfSQ35ykgc",
        "outputId": "1ade6ed4-f787-4ed6-fc68-1e881f5292d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-c502a44b2d79>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    smoking_types =\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "smoking_types = data['smoking_status'].value_counts()\n",
        "print(\"Categories and counts for 'smoking_status':\")\n",
        "print(smoking_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yg0xlLNUy7AF",
        "outputId": "d160e862-203f-4829-c631-b026d99a6deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-3242ce3e6048>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    residence_types =\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "residence_types = data['Residence_type'].value_counts()\n",
        "print(\"Categories and counts for 'Residence_type':\")\n",
        "print(residence_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T72r_mkrzF9V"
      },
      "outputs": [],
      "source": [
        "work_types =data['work_type'].value_counts()\n",
        "print(\"Categories and counts for 'work_type':\")\n",
        "print(work_types)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0kpR57LzQE9"
      },
      "outputs": [],
      "source": [
        "married_types =data['ever_married'].value_counts()\n",
        "print(\"Categories and counts for 'ever_married':\")\n",
        "print(married_types)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fQIoAs8y3igG",
        "outputId": "e7eb7335-5f47-468b-e42c-a61fafb50bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-70bbe8faed2d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    hypertension =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#naming of cell\n",
        "#hypertension =\n",
        "#hypertension\n",
        "hypertension_counts =data['hypertension'].value_counts()\n",
        "print(\"Categories and counts for 'hypertension':\")\n",
        "print(hypertension_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xEhNH46j3p3Y",
        "outputId": "a440ccd7-9901-46e2-82ad-20c07cf5ef76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-562dfc37378d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    heart_disease=\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#naming of cell\n",
        "#heart_disease=\n",
        "#heart_disease\n",
        "heart_disease_counts =data['heart_disease'].value_counts()\n",
        "print(\"Categories and counts for 'heart_disease':\")\n",
        "print(heart_disease_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8cI7uJvA3Njv",
        "outputId": "6f3ade91-a371-4049-ce7a-c5c234eb78ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-3f27925a05ae>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    stroke=\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#stroke=\n",
        "#stroke\n",
        "stroke_counts =data['stroke'].value_counts()\n",
        "print(\"Categories and counts for 'stroke':\")\n",
        "print(stroke_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jFIiKPvllld"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsQQj-wVlkjS"
      },
      "source": [
        "### Dealing with Nulls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdlRc672lgI4"
      },
      "source": [
        "**Question 4:** The bmi column contains nulls. Fill it with the appropriate measure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UDrnGK6lYiE"
      },
      "outputs": [],
      "source": [
        "data['bmi'].fillna(data['bmi'].mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21da4C_Bzd-u"
      },
      "source": [
        "#### Encoding Categorical Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaNk4gWCLqe4"
      },
      "source": [
        "**Question 5:** Here you have to encode those categorical variables to be able to use them to train your DL model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nJtOvxdzi_G"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "data['smoking_status'] = encoder.fit_transform(data['smoking_status'])\n",
        "data['Residence_type'] = encoder.fit_transform(data['Residence_type'])\n",
        "data['work_type'] = encoder.fit_transform(data['work_type'])\n",
        "data['ever_married'] = encoder.fit_transform(data['ever_married'])\n",
        "data['gender'] = encoder.fit_transform(data['gender'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1GOfAgt4M-Q"
      },
      "source": [
        "### Normalizing Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPpkMCXELwty"
      },
      "source": [
        "**Question 6:** Normalize the input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtI_XA-m33Bx"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "columns_to_normalize = ['age', 'avg_glucose_level', 'bmi']\n",
        "data[columns_to_normalize] = scaler.fit_transform(data[columns_to_normalize])\n",
        "print(data.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KICuY0lg5nUD"
      },
      "source": [
        "### Removing Unnecessary Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua71U2YxL5Ls"
      },
      "source": [
        "**Question 7:** From the features that you have, remove the feature(s) that is(are) irrelevant to your predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8AHNqYbh5sE7",
        "outputId": "11d2f3ee-b1a4-4059-ddc3-6ee3a0aa4917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-e65ee9838857>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    data = data.drop(    , axis=1)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "data = data.drop('id', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k-KoLpH5C9R"
      },
      "source": [
        "# Building the DL Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI2VtlafMBeN"
      },
      "source": [
        "**Question 8:** Now it's time to build the actual model, and observe a summary of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AZvKqqT65E0W",
        "outputId": "382d53c0-8b9e-4bac-d804-acea36e96843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-e4f342432336>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    input_dim = # Specify the number of input features\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "input_dim =12\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units=64, activation='relu', input_dim=input_dim))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD57fE2n7QP4"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seGmh1x1-qH9"
      },
      "source": [
        "**Question 9:**  Now we compile the model. Here we want to measure the accuracy as well as the precision and recall to know better about the performance of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woSsSTEm61_U"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5zevRH57X8v"
      },
      "source": [
        "### Fitting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhaEU26KMUWK"
      },
      "source": [
        "**Question 10:** Split the data and train the model\n",
        "\n",
        "We take the first columns as features and the last column as a label, then we split our dataset between training (70%) and testing (30%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rsVOVfn47MLn",
        "outputId": "5b26632e-6bd7-40fa-b00b-57773acdfbfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-89f8267bcbc2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x = data.iloc[:, :-1]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split the dataset into training (70%) and testing (30%) sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3, stratify=y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVvxbyPyezb3"
      },
      "source": [
        "we fit the model on 80% training data, and validate on the rest. Later we will do the final test on the test data. The training happens for 15 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrlvFubpqBeS"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voqUo00lAqCZ"
      },
      "source": [
        "# Improving DL Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 11:** Suggest ways to improve your model"
      ],
      "metadata": {
        "id": "Op2Frp44uSiX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IImSYWQGBSz6"
      },
      "source": [
        "### Checking For Data Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHxCONODMkNN"
      },
      "source": [
        "We check for imbalance because we have a poor recall and precision.\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bF6G8c58SOE"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=15, class_weight=class_weight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzZBB5GMCOld"
      },
      "source": [
        "We have a huge imbalance in the data, this is why we fix it with oversamppling and undersampling.\n",
        "\n",
        "We will oversample this time using the SMOTE() function instead of random oversampling, and this is because SMOTE will generate new data based on the data that we have, so we avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US4xON4LBX8I"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "over = SMOTE()\n",
        "x_new, y_new =\n",
        "\n",
        "\n",
        "plt.hist([y_new])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmfeUhi1ezb9"
      },
      "source": [
        "Split the balanced dataset between 90% (training and validation), 10% testing\n",
        "Then divide the 90% between 80% training and 20% validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxnYFoPZezb-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "x_train_val, x_test, y_train_val, y_test = train_test_split(x_new, y_new, test_size=0.1, stratify=y_new)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, stratify=y_train_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgyxF74oezcA"
      },
      "source": [
        "Now we will train the model on the balanced data, and tune it on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26YTispLEm9N"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAhUAu5JezcC"
      },
      "source": [
        "Evaluate your model on the test set that you kept aside at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utTyIgJLezcD"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhfhpIaWGtz2"
      },
      "source": [
        "We see that the performance gets better when our data became balanced.\n",
        "Now we will try improving our model with other techniques that we learned through the week."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngJVLbRKG7U_"
      },
      "source": [
        "### Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMdcXspCHxIo"
      },
      "source": [
        "We will introduce batch normalization after each layer and then train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK78-g-hHrmq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(32, input_dim=10, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(2, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REDrQVWLJLs5"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n",
        "\n",
        "history2 = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eenpPyr_ezcH"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhkSEThVKEdm"
      },
      "source": [
        "We see that we are achieving better metrics with batch normalization."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d1GOfAgt4M-Q",
        "KICuY0lg5nUD",
        "AD57fE2n7QP4",
        "U5zevRH57X8v",
        "IImSYWQGBSz6",
        "ngJVLbRKG7U_",
        "mXf6cpO_KWTs"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}